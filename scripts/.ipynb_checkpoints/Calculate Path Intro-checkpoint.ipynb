{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f340123-9870-4368-a787-8c74584b1e72",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b10bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shrinkbench.experiment import PruningExperiment, PruningClass\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b00629",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['ShrinkPATH'] = './shrinkbench'\n",
    "os.environ['DATAPATH'] = './shrinkbench/Training_data'\n",
    "# ShrinkPATH is the path from the directory this file is located to the shrinkbench code\n",
    "# DATAPATH is the path from the current directory to where the datasets are located\n",
    "# The only think you might need to change is 'shrinkbench' to whatever the name of the file is that \n",
    "# contains the shrinkbench code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9924ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressions = [4, 10, 20, 30, 40, 50]\n",
    "strategies = [\"GlobalMagWeight\"]\n",
    "# These are the compression ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6a0b3",
   "metadata": {},
   "source": [
    "This main block of code below should be rerun any time you switch between datasets and architectures. Otherwise does not need to be rerun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805c5cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGPU AVAILABLE\n",
      "\u001b[0m\u001b[92mGPU AVAILABLE\n",
      "\u001b[0m\u001b[95mLogging results to results/20220201-090040-OLGE-LeNet\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val -1/30: 100%|███████████████████████████████████| 79/79 [00:03<00:00, 23.04it/s, loss=0.018, top1=0.0842, top5=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('weight', Parameter containing:\n",
      "tensor([[[[ 0.1397, -0.2448, -0.1956],\n",
      "          [-0.0542,  0.1293,  0.2296],\n",
      "          [ 0.1355, -0.1561, -0.0236]]],\n",
      "\n",
      "\n",
      "        [[[-0.0630, -0.0294, -0.2096],\n",
      "          [ 0.0294,  0.2760,  0.0779],\n",
      "          [-0.1882,  0.3065,  0.2412]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3136,  0.2636, -0.2426],\n",
      "          [ 0.0542, -0.0632, -0.3189],\n",
      "          [-0.0443, -0.2858,  0.2214]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1770,  0.2044,  0.0023],\n",
      "          [ 0.1306, -0.1651, -0.1959],\n",
      "          [ 0.2933,  0.2913, -0.1921]]],\n",
      "\n",
      "\n",
      "        [[[ 0.2302,  0.1630, -0.0019],\n",
      "          [-0.0540, -0.1960,  0.0521],\n",
      "          [-0.2891,  0.2075, -0.0525]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0018,  0.0223, -0.1225],\n",
      "          [-0.0618, -0.1336,  0.1489],\n",
      "          [ 0.0778, -0.2231, -0.2564]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([-0.3006, -0.0160, -0.1990,  0.2503, -0.0277,  0.3086], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# This is the overarching object that is interacted with. \n",
    "exp = PruningClass(dataset='QMNIST', # Change this to 'Fashion', 'CIFAR10', or 'CIFAR100' \n",
    "                model='LeNet',  # LeNetChange this to 'resnet56' for CIFAR10, or 'resnet56_C' for CIFAR100\n",
    "                train_kwargs={\n",
    "                    'optim': 'SGD',\n",
    "                    'epochs': 30,\n",
    "                    'lr': 1e-2},\n",
    "                dl_kwargs={'batch_size':128},\n",
    "                save_freq=1)\n",
    "exp.run_init()  # Sets up some stuff, can ignore output\n",
    "print(list(exp.model.conv1.named_parameters())) # List weights of first layer in LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede172b2",
   "metadata": {},
   "source": [
    "Load a trained model before prune/finetune.\n",
    "\n",
    "This is specific for qmnist, but the naming scheme is the same for the other models<br>\n",
    "example file: qmnist3.pt <br>\n",
    "'qmnist' is the dataset the model is for<br>\n",
    "'3' is the model number (0-9)<br>\n",
    "When passing the name to the load_model() function, do not include the .pt at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dd9e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGPU AVAILABLE\n",
      "\u001b[0m[('weight', Parameter containing:\n",
      "tensor([[[[-0.3878, -0.3219, -0.2951],\n",
      "          [ 0.1081,  0.0065,  0.2044],\n",
      "          [-0.3937, -0.3368, -0.1223]]],\n",
      "\n",
      "\n",
      "        [[[ 0.8501,  0.4977,  0.8209],\n",
      "          [-0.1642, -0.0482, -0.2074],\n",
      "          [-0.8127, -0.4852, -0.4717]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4350,  0.5906, -0.2358],\n",
      "          [ 0.6677, -0.2312, -0.5057],\n",
      "          [ 0.0036, -0.4088, -0.2914]]],\n",
      "\n",
      "\n",
      "        [[[-0.2262,  0.1376,  0.2873],\n",
      "          [ 0.3484,  0.9641,  0.9536],\n",
      "          [ 0.4806,  0.9537,  0.9556]]],\n",
      "\n",
      "\n",
      "        [[[ 0.3661,  0.3191, -0.3480],\n",
      "          [-0.2950, -0.1044,  0.0255],\n",
      "          [-0.2242, -0.2711,  0.1878]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0548,  0.2241,  0.3392],\n",
      "          [-0.4242,  0.1320,  0.2449],\n",
      "          [-0.6496, -0.2265,  0.6873]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([ 0.3525,  0.1758, -0.1078,  0.5580, -0.1937,  0.0264], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = exp.load_model(\"qmnist1\")\n",
    "exp.build_model(\"LeNet\") # Change this when going between different model architectures. \n",
    "exp.to_device()\n",
    "exp.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "exp.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
    "print(list(exp.model.conv1.named_parameters())) # List weights of first layer in LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31afe80",
   "metadata": {},
   "source": [
    "Load a pruned/finetuned model.\n",
    "\n",
    "This is specific for qmnist, but the naming scheme is the same for the other models<br>\n",
    "example file: qmnist3.c30.pt <br>\n",
    "'qmnist' is the dataset the model is for<br>\n",
    "'3' is the model number (0-9)<br>\n",
    "c is whether it is after pruning or after finetuning. c for pruning and f for finetuning. <br>\n",
    "30 is what the compression ratio is. Can be 4, 10, 20, 30, 40, 50 for the CIFAR ones, or 4, 10, 20, 30, 40, 50 for <br>\n",
    "the QMNIST/Fashion ones <br>\n",
    "When passing the name to the load_model() function, do not include the .pt at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a58afa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGPU AVAILABLE\n",
      "\u001b[0m\u001b[92mModel Pruned using GlobalMagWeight strategy\n",
      "\u001b[0m\u001b[92mGPU AVAILABLE\n",
      "\u001b[0m[('weight', Parameter containing:\n",
      "tensor([[[[-0.4261, -0.3670, -0.3651],\n",
      "          [ 0.0000,  0.0000,  0.1755],\n",
      "          [-0.4350, -0.3564, -0.0000]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1285,  0.8205,  1.1275],\n",
      "          [-0.0000, -0.0000, -0.0000],\n",
      "          [-1.0843, -0.8787, -0.7332]]],\n",
      "\n",
      "\n",
      "        [[[ 0.6368,  0.7240, -0.3085],\n",
      "          [ 0.8393, -0.3091, -0.6965],\n",
      "          [ 0.0000, -0.5965, -0.3884]]],\n",
      "\n",
      "\n",
      "        [[[-0.0000,  0.1981,  0.4300],\n",
      "          [ 0.5450,  1.1346,  1.2012],\n",
      "          [ 0.7503,  1.2435,  1.3005]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4447,  0.3670, -0.3518],\n",
      "          [-0.3388, -0.0000,  0.0000],\n",
      "          [-0.2564, -0.2722,  0.2224]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0000,  0.4361,  0.4987],\n",
      "          [-0.6122,  0.1791,  0.5199],\n",
      "          [-0.9072, -0.3447,  0.9279]]]], device='cuda:0', requires_grad=True)), ('bias', Parameter containing:\n",
      "tensor([0.4347, 0.0000, 0.0000, 0.8149, -0.0000, 0.0000], device='cuda:0',\n",
      "       requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "checkpoint = exp.load_model(\"qmnist3.c30\")\n",
    "exp.build_model(\"LeNet\") # Change this when going between different model architectures.\n",
    "exp.to_device()\n",
    "\n",
    "exp.compression = 30\n",
    "exp.strategy = \"GlobalMagWeight\"\n",
    "exp.prune()\n",
    "exp.to_device()\n",
    "exp.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "exp.optim.load_state_dict(checkpoint['optim_state_dict'])\n",
    "print(list(exp.model.conv1.named_parameters())) # List weights of first layer in LeNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7e9075-2b33-4b90-aee7-39987f71a09b",
   "metadata": {},
   "source": [
    "`name_parameters()` returns weights at 0 and biases at 1. Here we only need weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6a9c2-a518-4744-9ac6-40ec3a6526ff",
   "metadata": {},
   "source": [
    "Output shape formula\n",
    "```\n",
    "[(W−K+2P)/S]+1.\n",
    "```\n",
    "* W is the input volume - in your case 128\n",
    "* K is the Kernel size - in your case 5\n",
    "* P is the padding - in your case 0 i believe\n",
    "* S is the stride - which you have not provided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347deb98-14a3-4ca6-9138-c4384f7d3480",
   "metadata": {},
   "source": [
    "## Calculate paths in linear neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6864d265-cb3b-40c7-ad4c-937d60c1f7ca",
   "metadata": {},
   "source": [
    "#### For calculate each layer\n",
    "This method contains two parameters:\n",
    "* Weights contains all the wights for all nodes at current layer, e.g. \n",
    "```python\n",
    "[[1,0,2,1],\n",
    " [1,1,0,0]]\n",
    "```\n",
    "* Previous path contains the number of path to previous layer of all nodes, e.g. \n",
    "```python\n",
    "[[1,5,2,8]]\n",
    "```\n",
    "\n",
    "The output for the example above would be `[11, 6]`, which is the number of paths for the input of the next layer.\n",
    "\n",
    "Note that the calculation start from the second layer, the path to the first layer is initialized to ones'array with the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0236bf98-702c-4c87-9968-cf85d1c36790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_linear_layer_paths(prev_paths, weights, threshold=None):\n",
    "    next_path = torch.tile(prev_paths, (weights.size()[0],)).reshape(weights.size())\n",
    "    if threshold == None:\n",
    "        next_path[weights == 0] = 0\n",
    "    else:\n",
    "        next_path[weights.abs() <= threshold] = 0\n",
    "    next_path = next_path.sum(axis=-1)\n",
    "    return next_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f02da2-5e93-40f1-8e65-7d485f7c50bc",
   "metadata": {},
   "source": [
    "![pic](src/linear_model.jpg)\n",
    "For the above simple model we can calculate the path as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f780a-0dd2-4a75-869d-4e7d1e519bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_paths = torch.tensor([1,1,1,1])\n",
    "weights = torch.tensor([[1,0,2,1], [1,1,0,0]])\n",
    "prev_paths = cal_linear_layer_paths(prev_paths, weights)\n",
    "prev_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d9004f-bacb-4a6e-9994-2dffd04c84d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3, 0, 5])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([[2,0], [0,0], [1,1]])\n",
    "prev_paths = cal_linear_layer_paths(prev_paths, weights)\n",
    "prev_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab93711-9f7a-4d68-a363-d6887bb95d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([8, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.tensor([[1,2,1], [0,0,1]])\n",
    "prev_paths = cal_linear_layer_paths(prev_paths, weights)\n",
    "prev_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93f1189-94a2-4ca3-b5e9-27b6900ec7f3",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate paths in linear neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925ecfcd-7375-4ec6-a6e6-e9c9faeea16f",
   "metadata": {},
   "source": [
    "#### For calculate each layer\n",
    "This method contains two parameters:\n",
    "* Weights contains all the wights for all nodes at current layer, e.g. \n",
    "```python\n",
    "weights = [[[1,0,1],\n",
    "            [2,3,0],\n",
    "            [2,0,0]],\n",
    "           [[1,0,1],\n",
    "            [2,3,1],\n",
    "            [0,1,1]]]\n",
    "```\n",
    "* Previous path contains the number of path to previous layer of all nodes, e.g. \n",
    "```python\n",
    "prev_paths = [2,1,1]\n",
    "```\n",
    "\n",
    "The calculation contains 3 steps:\n",
    "* calculate paths of current layer per channel, e.g. `[5,7]`\n",
    "* calculate the sum of all paths connect to current layer, e.g. `[4]`\n",
    "* assume fully connected, multiply the sum with paths in each channel in currently layer, e.g. `[20, 28]`\n",
    "\n",
    "The output for the example above would be `[20, 28]`, which is the number of paths for the input of the next layer.\n",
    "\n",
    "Note that the calculation start from the second layer, the path to the first layer is initialized to ones'array with the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f862c3b-27fb-4d55-898d-419d13e9e530",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor([[[1,0,1],\n",
    "            [2,3,0],\n",
    "            [2,0,0]],\n",
    "           [[1,0,1],\n",
    "            [2,3,1],\n",
    "            [0,1,1]]])\n",
    "prev_paths = torch.tensor([2,1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c82f40-73f2-4388-9746-59ea6cb62125",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_conv_layer_paths(prev_paths, weights):\n",
    "    # Calculate paths at current layer\n",
    "    cur_path = weights.reshape(len(weights),-1)\n",
    "    cur_path = (cur_path != 0).sum(axis=-1)\n",
    "    # Calculate sum of all previous paths\n",
    "    if prev_paths == None:\n",
    "        total_prev_paths = 1\n",
    "    else: \n",
    "        total_prev_paths = prev_paths.sum()\n",
    "    # Calculate next paths\n",
    "    next_path = cur_path * total_prev_paths\n",
    "    return next_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9581715-d4c0-4f2d-b3c4-ed54cda0ba67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15, 10])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cal_conv_layer_paths(prev_paths, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d49ae74-2516-4160-a69e-04c846dca057",
   "metadata": {},
   "source": [
    "## Expand paths counted from convolutional layer to linear layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7668d113-d057-4daa-bf0c-23ce3c7bbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expend_paths(prev_paths, weights):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    in_shape = weights.shape[-1]\n",
    "    path_mat = torch.zeros((in_shape // len(prev_paths) , len(prev_paths))).to(device)\n",
    "    path_mat[:,:] = prev_paths\n",
    "    return path_mat.T.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb42416d-b2f7-405c-aa27-21627ea1a608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, 23,  3])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.tensor([1,23,-3])\n",
    "k.abs()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c3fddf-1090-415d-8a0d-83ed0018e484",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate actual paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c164191f-57fa-4e57-b672-10e2734e32cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfbf26d5-c8d6-4a0c-bbcf-579fb60b8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LeNet(\n",
      "  (conv1): Conv2dMasked(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2dMasked(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): LinearMasked(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): LinearMasked(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(exp.model)\n",
    "cuda0 = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "817d4b56-ae7a-45d8-8380-59fd06fba61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isConv(layer):\n",
    "    c = str(type(layer)).lower()\n",
    "    return re.search(r'.conv*', c) != None\n",
    "def isLinear(layer):\n",
    "    c = str(type(layer)).lower()\n",
    "    return re.search(r'.linear*', c) != None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "968098a6-38a5-4ef8-bb15-14f084a17fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.randn(100, 1, 28, 28).to(cuda0)\n",
    "prev_paths = None\n",
    "prev_layer = None\n",
    "for layer in exp.model.modules():\n",
    "    cur_path = []\n",
    "    if isConv(layer):\n",
    "        weights = list(layer.named_parameters())[0][1]\n",
    "        cur_path = cal_conv_layer_paths(prev_paths, weights)\n",
    "        prev_layer = layer\n",
    "        prev_paths = cur_path\n",
    "    elif isLinear(layer):\n",
    "        weights = list(layer.named_parameters())[0][1]\n",
    "        if isConv(prev_layer):\n",
    "            prev_paths = expend_paths(prev_paths, weights)\n",
    "        cur_paths = cal_linear_layer_paths(prev_paths, weights, threshold = 0.2)\n",
    "        prev_layer = layer\n",
    "        prev_paths = cur_paths\n",
    "final = prev_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3e24cc45-e2ed-40a1-890d-7de1e1b21ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([199606., 199778., 261999., 266170., 231641., 203906., 237188., 222783.,\n",
       "        326499., 295281.], device='cuda:0')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a866dd0-e39b-4c5c-aa2f-87a5c942f2b5",
   "metadata": {},
   "source": [
    "## Plot Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "83b7c4cf-c986-4347-aa60-17fceb92bce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f19301d7340>]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(final.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213123c-bc28-4982-a836-13e25a084087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
